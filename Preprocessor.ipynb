{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47076c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edca6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_TYPE = '4G' #To decide which 3GPP files we are working on. Change this to '4G' if you want to process 4G files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1d8a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:66: DeprecationWarning: invalid escape sequence \\(\n",
      "<>:67: DeprecationWarning: invalid escape sequence \\)\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, output_path):\n",
    "        self.output_path = output_path\n",
    "        self.output_file = None\n",
    "        self.line_count = 0\n",
    "        self.file_count = 0\n",
    "        self.nas_end_idx = 0\n",
    "        \n",
    "    def find_sections(self, line):\n",
    "        section_header = re.compile(r'\\d(\\.\\d+)+[A-Za-z]*') #Find if the line is of pattern 3.4 or 5.7.1 or 4.6.7a, etc.\n",
    "        if section_header.match(line) is not None:\n",
    "            #print(line)\n",
    "            return True\n",
    "\n",
    "        \n",
    "    def processAll(self, input_path):\n",
    "        #input_path is a directory containing raw data\n",
    "        \n",
    "        self.file_count = 0\n",
    "        output_file_path = os.path.join(self.output_path,f'conflict_segments_{NET_TYPE}.txt') #these segments will be paired later to make\n",
    "                                                                                  #the final dataset \n",
    "        \n",
    "        if os.path.exists(output_file_path):\n",
    "            with open(output_file_path, 'r') as fp:\n",
    "                self.line_count = len(fp.readlines()) #To get the next sequence number, as we are appending\n",
    "            \n",
    "        self.output_file = open(output_file_path,'w')\n",
    "        f_count = 0\n",
    "        for file in glob.glob(input_path + '/**/*.txt', recursive=True):\n",
    "            print(os.path.basename(file))\n",
    "            #if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_path,os.path.basename(file))\n",
    "            print(file_path)\n",
    "            self.processIt(file_path, [1])\n",
    "            if f_count == 0:\n",
    "                self.output_file.write('---------File Ends-------\\n')\n",
    "            f_count += 1\n",
    "        self.output_file.close()\n",
    "    \n",
    "    def processIt(self, file, task:list):\n",
    "        \n",
    "        self.file_count += 1\n",
    "        file_line_count = 0\n",
    "        file_section_count = -1\n",
    "        #file: path to a file\n",
    "        with open(file) as f:\n",
    "            text = f.readlines()\n",
    "            for line in text:\n",
    "                #print(line)\n",
    "                if self.find_sections(line):\n",
    "                    self.output_file.write(\".\\n------\\n\")\n",
    "                    file_section_count += 1\n",
    "                    #new_section = True\n",
    "                    continue\n",
    "                if len(line.split()) < 4 : #skip the line\n",
    "                    continue\n",
    "                #if '((' or '))' or ':=' or '::' or '{{' or '}}' or '[[' or ']]' in line:\n",
    "                #    continue\n",
    "                if 'Editor\\'s Note' in line:\n",
    "                    continue\n",
    "                line = re.sub(r' +', ' ', line) #multiple whitespace, keep one\n",
    "                line = re.sub(r'(\\n)+', '', line) #remove extra newlines\n",
    "                line = re.sub(r'^[\\.Â·\\-]', '', line) #starting dot, interpunct, hyphen removal\n",
    "                line = re.sub(r'^ ', '', line) #starting whitespace removal\n",
    "                line = re.sub(r'[\\.:,;\\-]*$', '', line) #one or multiple dot, colon, hyphen, semicolon removal at the end\n",
    "                line = re.sub(r'(\\( )', '\\(', line) #whitespace after opening paren.\n",
    "                line = re.sub(r'( \\))', '\\)', line) #whitespace before closing paren.\n",
    "                line = re.sub(r'(\\(\\))|(\\[\\])|(\\{\\})', '', line) #remove empty paren, curly-braces, brackets\n",
    "                line = re.sub(r'(as shown below|[Ss]ee figure below):*\\-*', '', line) #remove certain strings\n",
    "                line = re.sub(r'([,;:])(\\w)', r'\\1 \\2', line) #insert whitespace after punctuations, except fullstop, underscore\n",
    "                                                              #hyphen\n",
    "                line = re.sub(r'\\ue000', '', line, re.UNICODE)\n",
    "                #line = line + '.'\n",
    "                \n",
    "                if line != '' and line != '.':\n",
    "                    line_set = line.split('. ') #Some lines still have multiple sentences\n",
    "                    for i, sentence in enumerate(line_set):\n",
    "                        self.line_count += 1\n",
    "                        file_line_count += 1\n",
    "                        if i != len(line_set)-1:\n",
    "                            #self.output_file.write(str(self.line_count) + ', $' + sentence + '.$\\n')\n",
    "                            self.output_file.write(sentence + '.\\n')\n",
    "                        else:\n",
    "                            #self.output_file.write(str(self.line_count) + ', $'+ sentence+'$' + '\\n')\n",
    "                            self.output_file.write(sentence+ \" \")\n",
    "                    #print('check')\n",
    "                #end of document\n",
    "            self.output_file.write('\\n')\n",
    "            print(str(self.file_count)+ \" file(s) completed. \" + str(file_section_count) + \" sections added. \" +  str(file_line_count) + \" lines added.\")\n",
    "            \n",
    "            #print(\"hey\", text)\n",
    "            print(\"\\n---------------------------------------------------------------------------------------------\\n\")\n",
    "        #task is a list of preprocessing tasks you want to carry on\n",
    "        #############add code for getlines###########\n",
    "    \n",
    "    def pairUp(self, file, num_sentences_per_article = 3):\n",
    "        corpus = []\n",
    "        data_string = \"\"\n",
    "        with open(file) as f:\n",
    "            text = f.readlines()\n",
    "            print(len(text))\n",
    "            num_lines = 0\n",
    "            for line in text:\n",
    "                if \"---------File Ends-------\" in line:\n",
    "                    self.nas_end_idx = len(corpus) #To check where the first file ends. Helps for disjoint dataset building\n",
    "                    continue\n",
    "                if line == '.\\n' or line == '.':\n",
    "                    continue\n",
    "                if line == '\\n':\n",
    "                    continue\n",
    "                if line == \"------\\n\":\n",
    "                    #print('yo')\n",
    "                    if data_string != \"\":\n",
    "                        corpus.append(data_string)\n",
    "                        num_lines = 0\n",
    "                    data_string = \"\"\n",
    "                else:\n",
    "                    data_string += line\n",
    "                    num_lines += 1\n",
    "                if num_lines == num_sentences_per_article or len(data_string.split()) > 50:\n",
    "                    corpus.append(data_string)\n",
    "                    data_string = \"\"\n",
    "                    num_lines = 0\n",
    "            if data_string != \"\": #The last one\n",
    "                corpus.append(data_string)\n",
    "                \n",
    "        #print(f'Corpus Length in number of sequences: {len(corpus)}')    \n",
    "        vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "        tfidf = vect.fit_transform(corpus)\n",
    "        #print(\"Shape of array: \",tfidf.shape)\n",
    "        pairwise_similarity = tfidf * tfidf.T\n",
    "        #print(pairwise_similarity)\n",
    "        return corpus, pairwise_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c7bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f\"./Data/Raw_{NET_TYPE}\" #all inputs\n",
    "\n",
    "output_path = f\"./Data/Processed_{NET_TYPE}\"\n",
    "os.makedirs(output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678889ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Security_4G.txt\n",
      "./Data/Raw_4G/Security_4G.txt\n",
      "1 file(s) completed. 120 sections added. 6416 lines added.\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "NAS_4G.txt\n",
      "./Data/Raw_4G/NAS_4G.txt\n",
      "2 file(s) completed. 890 sections added. 8475 lines added.\n",
      "\n",
      "---------------------------------------------------------------------------------------------\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataProcessor = Preprocessor(output_path)\n",
    "dataProcessor.processAll(input_path)\n",
    "print(dataProcessor.nas_end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e03996c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5323\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "corpus, pair_sim = dataProcessor.pairUp(os.path.join(output_path,f\"conflict_segments_{NET_TYPE}.txt\"))\n",
    "print(dataProcessor.nas_end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ba4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"./Data/cp_corpus_{NET_TYPE}.txt\", \"wb\") as fp:\n",
    "   pickle.dump(corpus, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
