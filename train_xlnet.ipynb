{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4429a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification, AdamW, XLNetConfig\n",
    "#from tokenizers import BertWordPieceTokenizer\n",
    "from torch import nn\n",
    "np.random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f879ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BertModel.from_pretrained(\"./Pretrained Models/bert_weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574e92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"~/Research/CellularLint\"\n",
    "DATA_PATH = \"./Data/SNLI/\"\n",
    "PRETRAINED_PATH = \"./Pretrained Models/xlnet_weights/\"\n",
    "SAVE_MODEL_AT = \"./saved_models/xlnet\"\n",
    "PRETRAINED_TOKENIZER = \"saved_models/xlnet\"\n",
    "MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7410e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATA_PATH,\"snli_1.0_train.csv\"))\n",
    "df_dev = pd.read_csv(os.path.join(DATA_PATH,\"snli_1.0_dev.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(DATA_PATH,\"snli_1.0_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb765ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.iloc[:500]\n",
    "df_dev = df_train.copy().iloc[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490b3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizerFast.from_pretrained(os.path.join(ROOT_PATH,PRETRAINED_TOKENIZER))\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(os.path.join(ROOT_PATH,PRETRAINED_PATH))\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07450df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'contradiction':1,\n",
    "          'entailment':0,\n",
    "          'neutral':2,\n",
    "          }\n",
    "NUM_LABELS = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37350522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlnet_encoding(sequence):\n",
    "    return tokenizer.encode(sequence, add_special_tokens = False)\n",
    "def str_to_int_list(data):\n",
    "    return list(map(int, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6043cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token_type'] = df_train['token_type'].str.split()\n",
    "df_train['token_type'] = df_train['token_type'].apply(str_to_int_list)\n",
    "\n",
    "df_dev['token_type'] = df_dev['token_type'].str.split()\n",
    "df_dev['token_type'] = df_dev['token_type'].apply(str_to_int_list)\n",
    "\n",
    "df_test['token_type'] = df_test['token_type'].str.split()\n",
    "df_test['token_type'] = df_test['token_type'].apply(str_to_int_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa55867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['attention_mask'] = df_train['attention_mask'].str.split()\n",
    "df_train['attention_mask'] = df_train['attention_mask'].apply(str_to_int_list)\n",
    "\n",
    "df_dev['attention_mask'] = df_dev['attention_mask'].str.split()\n",
    "df_dev['attention_mask'] = df_dev['attention_mask'].apply(str_to_int_list)\n",
    "\n",
    "df_test['attention_mask'] = df_test['attention_mask'].str.split()\n",
    "df_test['attention_mask'] = df_test['attention_mask'].apply(str_to_int_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_ids'] = df_train['sequence'].apply(xlnet_encoding)\n",
    "df_dev['input_ids'] = df_dev['sequence'].apply(xlnet_encoding)\n",
    "df_test['input_ids'] = df_test['sequence'].apply(xlnet_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e645695",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = []\n",
    "        self.labels = [labels[label] for label in df['gold_label']]\n",
    "        for _, row in df.iterrows():\n",
    "            token_type_ids = row['token_type']\n",
    "            token_type_ids += [0] * (max_length - len(token_type_ids))\n",
    "            attention_mask = row['attention_mask']\n",
    "            attention_mask += [0] * (max_length - len(attention_mask))\n",
    "            input_ids = tokenizer.encode(\n",
    "                row['sequence'],\n",
    "                add_special_tokens=False,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            datadict = {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'token_type_ids': torch.tensor(token_type_ids),\n",
    "                'attention_mask': torch.tensor(attention_mask)\n",
    "            }\n",
    "            self.texts.append(datadict)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_text = self.texts[idx]\n",
    "        batch_y = self.labels[idx]\n",
    "        return batch_text, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f50bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, load_path = None, dropout=0.5):\n",
    "\n",
    "        super(XLNetClassifier, self).__init__()\n",
    "\n",
    "        #self.bert = BertModel.from_pretrained(os.path.join(ROOT_PATH, load_path))\n",
    "        self.xlnet = XLNetModel.from_pretrained(load_path)\n",
    "        \n",
    "        #self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, NUM_LABELS)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_id, mask, token_type_id):\n",
    "        output = self.xlnet(input_ids= input_id, attention_mask = mask, token_type_ids = token_type_id,return_dict = False)\n",
    "        #_, pooled_output = self.xlnet(input_ids= input_id, attention_mask = mask, return_dict = False)\n",
    "        \n",
    "        pooled_output = output[0][:,-1,:] #Representation from last token\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        \n",
    "        linear_output = self.linear(dropout_output)\n",
    "        \n",
    "        final_layer = self.softmax(linear_output)\n",
    "        \n",
    "        return final_layer\n",
    "    \n",
    "    def save(self, save_dir, tokenizer, model_name = \"model_xlnet.pt\"):\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Save model weights\n",
    "        #model_path = os.path.join(save_dir, model_name)\n",
    "        #torch.save(self.state_dict(), model_path)\n",
    "        self.xlnet.save_pretrained(save_dir)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        # Save other related information\n",
    "        #config_path = os.path.join(save_dir, \"config.json\")\n",
    "        #self.xlnet.config.to_json_file(config_path)\n",
    "\n",
    "    def load(self, load_dir, is_eval = True, model_name = \"model_xlnet.pt\"):\n",
    "        # Load tokenizer\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(load_dir)\n",
    "\n",
    "        # Load other related information\n",
    "        config_path = os.path.join(load_dir, \"config.json\")\n",
    "        config = XLNetConfig.from_json_file(config_path)\n",
    "        self.xlnet = XLNetModel(config)\n",
    "        if is_eval:\n",
    "            self.xlnet.eval()  # Set to evaluation mode\n",
    "\n",
    "        # Load model weights\n",
    "        model_path = os.path.join(load_dir, model_name)\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        # Update the tokenizer\n",
    "        self.xlnet.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37296007",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Pretrained Models/xlnet_weights/ were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/rahman75/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.038             | Train Accuracy:  0.348             | Val Loss:  0.043             | Val Accuracy:  0.275\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.038             | Train Accuracy:  0.314             | Val Loss:  0.043             | Val Accuracy:  0.285\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.037             | Train Accuracy:  0.360             | Val Loss:  0.041             | Val Accuracy:  0.365\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.038             | Train Accuracy:  0.332             | Val Loss:  0.042             | Val Accuracy:  0.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.038             | Train Accuracy:  0.338             | Val Loss:  0.041             | Val Accuracy:  0.330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.038             | Train Accuracy:  0.316             | Val Loss:  0.042             | Val Accuracy:  0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.038             | Train Accuracy:  0.330             | Val Loss:  0.041             | Val Accuracy:  0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:24<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.038             | Train Accuracy:  0.338             | Val Loss:  0.040             | Val Accuracy:  0.370\n",
      "Found a better model\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "learning_rates = [1e-5] #[5e-6, 1e-5, 2e-5, 3e-5, 5e-5]\n",
    "batch_sizes = [8] #[16, 24, 32, 40]\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, batch_size):\n",
    "    #out_file.write(f\"-------------Starting with LR = {learning_rate} and BS = {batch_size}-----------------\\n\")\n",
    "    best_acc_val = -99999\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr= learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "                #print(train_input)\n",
    "                train_label = train_label.to(device)\n",
    "                \n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "                token_type_id = train_input['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, token_type_id)\n",
    "                #logits = output.logits #For BertForSequenceClassification\n",
    "                logits = output\n",
    "                #print(f'training logits: {logits}')\n",
    "                #print(\"training outputs\")\n",
    "                #print(output)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batch_loss = criterion(logits, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                #print(f'prediction: {torch.argmax(logits, dim=1)}')\n",
    "                #print(f\"train labels: {train_label}\")\n",
    "                acc = (torch.argmax(logits, dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = val_input['attention_mask'].squeeze(1).to(device)\n",
    "                token_type_id = val_input['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, token_type_id)\n",
    "                #logits = output.logits #For BertForSequenceClassification\n",
    "                logits = output\n",
    "                #print(val_label)\n",
    "                #print(f'validating logits: {logits}')\n",
    "                #print(f'prediction: {torch.argmax(logits, dim=1)}')\n",
    "\n",
    "                batch_loss = criterion(logits, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "                #print(output)\n",
    "                acc = (torch.argmax(logits, dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        if(total_acc_val / len(val_data) > best_acc_val):\n",
    "            best_acc_val = total_acc_val / len(val_data)\n",
    "            #model.bert.save_pretrained(os.path.join(ROOT_PATH,\"saved_models/finetune_p1/bert/\"))\n",
    "            #tokenizer.save_pretrained(os.path.join(ROOT_PATH,\"saved_models/finetune_p1/bert/\"))\n",
    "\n",
    "            model.save(save_dir = SAVE_MODEL_AT, tokenizer = tokenizer)\n",
    "            print(\"Found a better model\")\n",
    "\n",
    "EPOCHS = 8\n",
    "for LR in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        #model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = NUM_LABELS)\n",
    "        model = XLNetClassifier(load_path = PRETRAINED_PATH)\n",
    "        train(model, df_train, df_dev, LR, EPOCHS, bs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
