{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4429a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, BertConfig, BertTokenizerFast\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch import nn\n",
    "np.random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35f879ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Pretrained Models/bert_weights/ were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./Pretrained Models/bert_weights/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BertModel.from_pretrained(\"./Pretrained Models/bert_weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574e92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"~/Research/CellularLint\"\n",
    "DATA_PATH = os.path.join(ROOT_PATH,\"Data/SNLI/\")\n",
    "PRETRAINED_PATH = \"./Pretrained Models/bert_weights/\"\n",
    "SAVE_MODEL_AT = \"./saved_models/finetune/bert_uncased\"\n",
    "PRETRAINED_TOKENIZER = \"saved_models/tokenizer/bert_uncased\"\n",
    "MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7410e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATA_PATH,\"snli_1.0_train.csv\"))\n",
    "df_dev = pd.read_csv(os.path.join(DATA_PATH,\"snli_1.0_dev.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(DATA_PATH,\"snli_1.0_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb765ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.iloc[:500]\n",
    "df_dev = df_train.copy().iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "490b3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizerFast.from_pretrained(os.path.join(ROOT_PATH,PRETRAINED_TOKENIZER))\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(os.path.join(ROOT_PATH,PRETRAINED_PATH))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07450df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'contradiction':1,\n",
    "          'entailment':0,\n",
    "          'neutral':2,\n",
    "          }\n",
    "NUM_LABELS = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37350522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encoding(sequence):\n",
    "    return tokenizer.encode(sequence, add_special_tokens = False)\n",
    "def str_to_int_list(data):\n",
    "    return list(map(int, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6043cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token_type'] = df_train['token_type'].str.split()\n",
    "df_train['token_type'] = df_train['token_type'].apply(str_to_int_list)\n",
    "\n",
    "df_dev['token_type'] = df_dev['token_type'].str.split()\n",
    "df_dev['token_type'] = df_dev['token_type'].apply(str_to_int_list)\n",
    "\n",
    "df_test['token_type'] = df_test['token_type'].str.split()\n",
    "df_test['token_type'] = df_test['token_type'].apply(str_to_int_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fa55867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['attention_mask'] = df_train['attention_mask'].str.split()\n",
    "df_train['attention_mask'] = df_train['attention_mask'].apply(str_to_int_list)\n",
    "\n",
    "df_dev['attention_mask'] = df_dev['attention_mask'].str.split()\n",
    "df_dev['attention_mask'] = df_dev['attention_mask'].apply(str_to_int_list)\n",
    "\n",
    "df_test['attention_mask'] = df_test['attention_mask'].str.split()\n",
    "df_test['attention_mask'] = df_test['attention_mask'].apply(str_to_int_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff2731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_ids'] = df_train['sequence'].apply(bert_encoding)\n",
    "df_dev['input_ids'] = df_dev['sequence'].apply(bert_encoding)\n",
    "df_test['input_ids'] = df_test['sequence'].apply(bert_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e645695",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = []\n",
    "        self.labels = [labels[label] for label in df['gold_label']]\n",
    "        for _, row in df.iterrows():\n",
    "            token_type_ids = row['token_type']\n",
    "            token_type_ids += [0] * (max_length - len(token_type_ids))\n",
    "            attention_mask = row['attention_mask']\n",
    "            attention_mask += [0] * (max_length - len(attention_mask))\n",
    "            input_ids = tokenizer.encode(\n",
    "                row['sequence'],\n",
    "                add_special_tokens=False,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            datadict = {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'token_type_ids': torch.tensor(token_type_ids),\n",
    "                'attention_mask': torch.tensor(attention_mask)\n",
    "            }\n",
    "            self.texts.append(datadict)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_text = self.texts[idx]\n",
    "        batch_y = self.labels[idx]\n",
    "        return batch_text, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8f50bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, load_path = None, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        #self.bert = BertModel.from_pretrained(os.path.join(ROOT_PATH, load_path))\n",
    "        self.bert = BertModel.from_pretrained(load_path)\n",
    "        \n",
    "        #self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, NUM_LABELS)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_id, mask, token_type_id):\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask = mask, token_type_ids = token_type_id,return_dict = False)\n",
    "        \n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        \n",
    "        linear_output = self.linear(dropout_output)\n",
    "        \n",
    "        final_layer = self.softmax(linear_output)\n",
    "        \n",
    "        return final_layer\n",
    "    \n",
    "    def save(self, save_dir, tokenizer, model_name = \"model_3e-5_32.pt\"):\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Save model weights\n",
    "        model_path = os.path.join(save_dir, model_name)\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "        # Save tokenizer\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        # Save other related information\n",
    "        config_path = os.path.join(save_dir, \"config.json\")\n",
    "        self.bert.config.to_json_file(config_path)\n",
    "\n",
    "    def load(self, load_dir, is_eval = True, model_name = \"model_3e-5_32.pt\"):\n",
    "        # Load tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained(load_dir)\n",
    "\n",
    "        # Load other related information\n",
    "        config_path = os.path.join(load_dir, \"config.json\")\n",
    "        config = BertConfig.from_json_file(config_path)\n",
    "        self.bert = BertModel(config)\n",
    "        if is_eval:\n",
    "            self.bert.eval()  # Set to evaluation mode\n",
    "\n",
    "        # Load model weights\n",
    "        model_path = os.path.join(load_dir, model_name)\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        # Update the tokenizer\n",
    "        self.bert.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37296007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Pretrained Models/bert_weights/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./Pretrained Models/bert_weights/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rahman75/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.036             | Train Accuracy:  0.324             | Val Loss:  0.042             | Val Accuracy:  0.350\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.036             | Train Accuracy:  0.354             | Val Loss:  0.044             | Val Accuracy:  0.400\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.035             | Train Accuracy:  0.342             | Val Loss:  0.043             | Val Accuracy:  0.370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:09<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.035             | Train Accuracy:  0.336             | Val Loss:  0.044             | Val Accuracy:  0.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████▎                    | 12/16 [00:07<00:02,  1.66it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "learning_rates = [3e-5] #[5e-6, 1e-5, 2e-5, 3e-5, 5e-5]\n",
    "batch_sizes = [32] #[16, 24, 32, 40]\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, batch_size):\n",
    "    #out_file.write(f\"-------------Starting with LR = {learning_rate} and BS = {batch_size}-----------------\\n\")\n",
    "    best_acc_val = -99999\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr= learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "                #print(train_input)\n",
    "                train_label = train_label.to(device)\n",
    "                \n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "                token_type_id = train_input['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, token_type_id)\n",
    "                #logits = output.logits #For BertForSequenceClassification\n",
    "                logits = output\n",
    "                #print(f'training logits: {logits}')\n",
    "                #print(\"training outputs\")\n",
    "                #print(output)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batch_loss = criterion(logits, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                #print(f'prediction: {torch.argmax(logits, dim=1)}')\n",
    "                #print(f\"train labels: {train_label}\")\n",
    "                acc = (torch.argmax(logits, dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = val_input['attention_mask'].squeeze(1).to(device)\n",
    "                token_type_id = val_input['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, token_type_id)\n",
    "                #logits = output.logits #For BertForSequenceClassification\n",
    "                logits = output\n",
    "                #print(val_label)\n",
    "                #print(f'validating logits: {logits}')\n",
    "                #print(f'prediction: {torch.argmax(logits, dim=1)}')\n",
    "\n",
    "                batch_loss = criterion(logits, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "                #print(output)\n",
    "                acc = (torch.argmax(logits, dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        if(total_acc_val / len(val_data) > best_acc_val):\n",
    "            best_acc_val = total_acc_val / len(val_data)\n",
    "            #model.bert.save_pretrained(os.path.join(ROOT_PATH,\"saved_models/finetune_p1/bert/\"))\n",
    "            #tokenizer.save_pretrained(os.path.join(ROOT_PATH,\"saved_models/finetune_p1/bert/\"))\n",
    "\n",
    "            #model.save(save_dir = os.path.join(ROOT_PATH, SAVE_MODEL_AT), tokenizer = tokenizer)\n",
    "            print(\"Found a better model\")\n",
    "\n",
    "EPOCHS = 8\n",
    "for LR in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        #model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = NUM_LABELS)\n",
    "        model = BertClassifier(load_path = PRETRAINED_PATH)\n",
    "        train(model, df_train, df_dev, LR, EPOCHS, bs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
