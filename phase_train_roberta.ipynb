{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4429a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification, AdamW, RobertaConfig, RobertaTokenizerFast\n",
    "#from tokenizers import BertWordPieceTokenizer\n",
    "from torch import nn\n",
    "np.random.seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f879ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BertModel.from_pretrained(\"./Pretrained Models/bert_weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574e92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"~/Research/CellularLint\"\n",
    "DATA_PATH = \"./Data/Cellular/\"\n",
    "PRETRAINED_PATH = \"./Pretrained Models/roberta_weights/saved/\"\n",
    "SAVE_MODEL_AT = \"./saved_models/roberta\"\n",
    "PHASE_MODEL_AT = \"./phase_models/roberta\"\n",
    "PRETRAINED_TOKENIZER = \"saved_models/roberta\"\n",
    "MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7410e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATA_PATH,\"phase1-mini.csv\"))\n",
    "df_dev = pd.read_csv(os.path.join(DATA_PATH,\"phase1-mini.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(DATA_PATH,\"phase1-mini.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcb765ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df_train.iloc[:500]\n",
    "#df_dev = df_train.copy().iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490b3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizerFast.from_pretrained(os.path.join(ROOT_PATH,PRETRAINED_TOKENIZER))\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(os.path.join(ROOT_PATH,PRETRAINED_PATH))\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(SAVE_MODEL_AT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07450df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'contradiction':1,\n",
    "          'entailment':0,\n",
    "          'neutral':2,\n",
    "          }\n",
    "NUM_LABELS = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37350522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_encoding(sequence):\n",
    "    return tokenizer.encode(sequence, add_special_tokens = False)\n",
    "def str_to_int_list(data):\n",
    "    return list(map(int, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6043cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token_type'] = df_train['token_type'].str.split()\n",
    "df_train['token_type'] = df_train['token_type'].apply(str_to_int_list)\n",
    "\n",
    "df_dev['token_type'] = df_dev['token_type'].str.split()\n",
    "df_dev['token_type'] = df_dev['token_type'].apply(str_to_int_list)\n",
    "\n",
    "df_test['token_type'] = df_test['token_type'].str.split()\n",
    "df_test['token_type'] = df_test['token_type'].apply(str_to_int_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa55867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['attention_mask'] = df_train['attention_mask'].str.split()\n",
    "df_train['attention_mask'] = df_train['attention_mask'].apply(str_to_int_list)\n",
    "\n",
    "df_dev['attention_mask'] = df_dev['attention_mask'].str.split()\n",
    "df_dev['attention_mask'] = df_dev['attention_mask'].apply(str_to_int_list)\n",
    "\n",
    "df_test['attention_mask'] = df_test['attention_mask'].str.split()\n",
    "df_test['attention_mask'] = df_test['attention_mask'].apply(str_to_int_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_ids'] = df_train['sequence'].apply(roberta_encoding)\n",
    "df_dev['input_ids'] = df_dev['sequence'].apply(roberta_encoding)\n",
    "df_test['input_ids'] = df_test['sequence'].apply(roberta_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e645695",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = []\n",
    "        self.labels = [labels[label] for label in df['gold_label']]\n",
    "        for _, row in df.iterrows():\n",
    "            token_type_ids = row['token_type']\n",
    "            token_type_ids += [0] * (max_length - len(token_type_ids))\n",
    "            attention_mask = row['attention_mask']\n",
    "            attention_mask += [0] * (max_length - len(attention_mask))\n",
    "            input_ids = tokenizer.encode(\n",
    "                row['sequence'],\n",
    "                add_special_tokens=False,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            datadict = {\n",
    "                'input_ids': input_ids.squeeze(0),\n",
    "                'token_type_ids': torch.tensor(token_type_ids),\n",
    "                'attention_mask': torch.tensor(attention_mask)\n",
    "            }\n",
    "            self.texts.append(datadict)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_text = self.texts[idx]\n",
    "        batch_y = self.labels[idx]\n",
    "        return batch_text, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f50bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, load_path = None, dropout=0.5):\n",
    "\n",
    "        super(RobertaClassifier, self).__init__()\n",
    "\n",
    "        #self.bert = BertModel.from_pretrained(os.path.join(ROOT_PATH, load_path))\n",
    "        self.roberta = RobertaModel.from_pretrained(load_path)\n",
    "        \n",
    "        #self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, NUM_LABELS)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_id, mask, token_type_id):\n",
    "        _, pooled_output = self.roberta(input_ids= input_id, attention_mask = mask, return_dict = False)\n",
    "        \n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        \n",
    "        linear_output = self.linear(dropout_output)\n",
    "        \n",
    "        final_layer = self.softmax(linear_output)\n",
    "        \n",
    "        return final_layer\n",
    "    \n",
    "    def save(self, save_dir, tokenizer, model_name = \"model_roberta.pt\"):\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Save model weights\n",
    "        #model_path = os.path.join(save_dir, model_name)\n",
    "        #torch.save(self.state_dict(), model_path)\n",
    "        self.roberta.save_pretrained(save_dir)\n",
    "\n",
    "        # Save tokenizer\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        # Save other related information\n",
    "        #config_path = os.path.join(save_dir, \"config.json\")\n",
    "        #self.roberta.config.to_json_file(config_path)\n",
    "\n",
    "    def load(self, load_dir, is_eval = True, model_name = \"model_roberta.pt\"):\n",
    "        # Load tokenizer\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(load_dir)\n",
    "\n",
    "        # Load other related information\n",
    "        config_path = os.path.join(load_dir, \"config.json\")\n",
    "        config = RobertaConfig.from_json_file(config_path)\n",
    "        self.roberta = RobertaModel(config)\n",
    "        if is_eval:\n",
    "            self.roberta.eval()  # Set to evaluation mode\n",
    "\n",
    "        # Load model weights\n",
    "        model_path = os.path.join(load_dir, model_name)\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        # Update the tokenizer\n",
    "        self.roberta.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37296007",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahman75/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.138             | Train Accuracy:  0.418             | Val Loss:  0.136             | Val Accuracy:  0.473\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.135             | Train Accuracy:  0.509             | Val Loss:  0.135             | Val Accuracy:  0.491\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.133             | Train Accuracy:  0.527             | Val Loss:  0.132             | Val Accuracy:  0.527\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.133             | Train Accuracy:  0.509             | Val Loss:  0.131             | Val Accuracy:  0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.133             | Train Accuracy:  0.436             | Val Loss:  0.128             | Val Accuracy:  0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.129             | Train Accuracy:  0.509             | Val Loss:  0.124             | Val Accuracy:  0.655\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.123             | Train Accuracy:  0.655             | Val Loss:  0.120             | Val Accuracy:  0.673\n",
      "Found a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.121             | Train Accuracy:  0.636             | Val Loss:  0.116             | Val Accuracy:  0.673\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "learning_rates = [1e-5] #[5e-6, 1e-5, 2e-5, 3e-5, 5e-5]\n",
    "batch_sizes = [8] #[16, 24, 32, 40]\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, batch_size):\n",
    "    #out_file.write(f\"-------------Starting with LR = {learning_rate} and BS = {batch_size}-----------------\\n\")\n",
    "    best_acc_val = -99999\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr= learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "                #print(train_input)\n",
    "                train_label = train_label.to(device)\n",
    "                \n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = train_input['attention_mask'].squeeze(1).to(device)\n",
    "                token_type_id = train_input['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, token_type_id)\n",
    "                #logits = output.logits #For BertForSequenceClassification\n",
    "                logits = output\n",
    "                #print(f'training logits: {logits}')\n",
    "                #print(\"training outputs\")\n",
    "                #print(output)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batch_loss = criterion(logits, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                #print(f'prediction: {torch.argmax(logits, dim=1)}')\n",
    "                #print(f\"train labels: {train_label}\")\n",
    "                acc = (torch.argmax(logits, dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                \n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                mask = val_input['attention_mask'].squeeze(1).to(device)\n",
    "                token_type_id = val_input['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, token_type_id)\n",
    "                #logits = output.logits #For BertForSequenceClassification\n",
    "                logits = output\n",
    "                #print(val_label)\n",
    "                #print(f'validating logits: {logits}')\n",
    "                #print(f'prediction: {torch.argmax(logits, dim=1)}')\n",
    "\n",
    "                batch_loss = criterion(logits, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "                #print(output)\n",
    "                acc = (torch.argmax(logits, dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        if(total_acc_val / len(val_data) > best_acc_val):\n",
    "            best_acc_val = total_acc_val / len(val_data)\n",
    "            \n",
    "            model.save(save_dir = PHASE_MODEL_AT, tokenizer = tokenizer)\n",
    "            print(\"Found a better model\")\n",
    "\n",
    "EPOCHS = 8\n",
    "for LR in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        #model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = NUM_LABELS)\n",
    "        model = RobertaClassifier(load_path = SAVE_MODEL_AT)\n",
    "        train(model, df_train, df_dev, LR, EPOCHS, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c8d81",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1953181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = model = RobertaClassifier(load_path = PHASE_MODEL_AT)\n",
    "model = model.cuda()\n",
    "model.eval()  # Setting the model to evaluation mode\n",
    "test = Dataset(df_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_sizes[0])\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch, label in test_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)  # Remove the extra dimension added by DataLoader\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n",
    "        token_type_id = batch['token_type_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, token_type_id)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())  \n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "with open('./eval/roberta_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
